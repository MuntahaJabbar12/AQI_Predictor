"""
Training Pipeline - Main Script (UPDATED for 3-Month Data)
============================================================
This script trains ML models to predict AQI.

UPDATED: Now uses combined historical + real-time data if available!

Steps:
1. Load features (combined CSV or Hopsworks)
2. Prepare data for training
3. Train multiple models
4. Evaluate and compare
5. Save best model to Hopsworks Model Registry
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime
import joblib
import warnings
warnings.filterwarnings('ignore')

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from feature_pipeline.hopsworks_utils import (
    connect_to_hopsworks, 
    get_features_for_training,
    upload_model_to_registry
)
from feature_pipeline.feature_engineering import prepare_features_for_training

# ML Libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor

# Advanced models (optional)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è  XGBoost not available")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è  LightGBM not available")


def calculate_metrics(y_true, y_pred, model_name="Model"):
    """
    Calculate all evaluation metrics.
    
    Args:
        y_true: True values
        y_pred: Predicted values
        model_name: Name of the model
    
    Returns:
        Dictionary with metrics
    """
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # MAPE (Mean Absolute Percentage Error)
    # Avoid division by zero
    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1, y_true))) * 100
    
    metrics = {
        'model': model_name,
        'RMSE': round(rmse, 4),
        'MAE': round(mae, 4),
        'R2': round(r2, 4),
        'MAPE': round(mape, 4)
    }
    
    return metrics


def load_and_prepare_data(project):
    """
    Load data from combined CSV (if available) or Hopsworks.
    
    UPDATED: Checks for combined_aqi_data.csv first!
    
    Args:
        project: Hopsworks project object
    
    Returns:
        X_train, X_test, y_train, y_test, scaler, feature_names
    """
    print("\n" + "="*60)
    print("üìä LOADING AND PREPARING DATA")
    print("="*60 + "\n")
    
    # UPDATED: Check for combined data first
    combined_files = [
        '../combined_aqi_data.csv',
        'combined_aqi_data.csv',
        os.path.join(os.path.dirname(__file__), '..', 'combined_aqi_data.csv')
    ]
    
    df = None
    data_source = None
    
    # Try loading combined data
    for filepath in combined_files:
        if os.path.exists(filepath):
            print(f"Step 1/5: Loading combined data from {os.path.basename(filepath)}...")
            try:
                df = pd.read_csv(filepath)
                # FIXED: Use ISO8601 format for timestamp parsing
                df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')
                data_source = "Combined (Historical + Real-time)"
                print(f"‚úì Found and loaded combined dataset!")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è  Error loading {filepath}: {str(e)}")
                continue
    
    # Fallback to Hopsworks if combined data not found
    if df is None:
        print("Step 1/5: Loading features from Hopsworks...")
        print("   (No combined_aqi_data.csv found - using real-time data only)")
        df = get_features_for_training(project, feature_group_name="aqi_features", version=1)
        data_source = "Real-time only (Hopsworks)"
    
    if df is None or len(df) == 0:
        print("‚úó No data available!")
        return None, None, None, None, None, None, None, None
    
    # Ensure timestamp is datetime (FIXED: with ISO8601 format)
    if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')
    
    # Calculate duration
    duration_days = (df['timestamp'].max() - df['timestamp'].min()).days
    
    print(f"‚úì Loaded {len(df)} records")
    print(f"  Data source: {data_source}")
    print(f"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
    print(f"  Duration: {duration_days} days")
    
    # Check if meets 3-month requirement
    if duration_days >= 90:
        print(f"  ‚úÖ ‚úÖ ‚úÖ MEETS 3-MONTH REQUIREMENT! ({duration_days} days) ‚úÖ ‚úÖ ‚úÖ")
    elif duration_days >= 30:
        print(f"  ‚ö†Ô∏è  {duration_days} days (target: 90+ days for best results)")
    else:
        print(f"  ‚ö†Ô∏è  Only {duration_days} days - more data recommended")
    
    # Remove records with placeholder pollution data (aqi = 0)
    print("\nStep 2/5: Cleaning data...")
    df_clean = df[df['aqi'] > 0].copy()
    print(f"‚úì Kept {len(df_clean)} records with real AQI data")
    print(f"  (Removed {len(df) - len(df_clean)} placeholder records)")
    
    if len(df_clean) < 50:
        print("\n‚ö†Ô∏è  WARNING: Very few records with real pollution data!")
        print("   Recommendation: Run the hourly pipeline more to collect data")
        print("   For now, we'll use all available data including placeholders")
        df_clean = df.copy()
    
    # Sort by timestamp
    df_clean = df_clean.sort_values('timestamp').reset_index(drop=True)
    
    # Prepare features for training (adds lag and rolling features)
    print("\nStep 3/5: Engineering features...")
    df_features = prepare_features_for_training(df_clean)
    print(f"‚úì Created {len(df_features.columns)} total columns")
    
    # Define feature columns and target
    feature_cols = [
        # Current weather
        'temperature', 'humidity', 'pressure', 'wind_speed',
        
        # Current pollution
        'pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3',
        
        # Time features
        'hour', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour',
        
        # Derived features
        'pm_ratio', 'temp_humidity_interaction',
        
        # Lag features (1, 3, 6 hours)
        'pm2_5_lag_1', 'pm2_5_lag_3', 'pm2_5_lag_6',
        'pm10_lag_1', 'pm10_lag_3', 'pm10_lag_6',
        'aqi_lag_1', 'aqi_lag_3', 'aqi_lag_6',
        
        # Rolling features (3, 6 hour windows)
        'pm2_5_rolling_mean_3', 'pm2_5_rolling_mean_6',
        'pm10_rolling_mean_3', 'pm10_rolling_mean_6',
        'aqi_rolling_mean_3', 'aqi_rolling_mean_6',
        
        # AQI change rate
        'aqi_change_rate', 'aqi_change_rate_pct'
    ]
    
    # Keep only features that exist in the dataframe
    available_features = [col for col in feature_cols if col in df_features.columns]
    print(f"‚úì Using {len(available_features)} features for training")
    
    target_col = 'aqi'
    
    # Create feature matrix and target
    X = df_features[available_features].copy()
    y = df_features[target_col].copy()
    
    # Handle any remaining NaN values
    X = X.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    print(f"\nData shape:")
    print(f"  Features (X): {X.shape}")
    print(f"  Target (y): {y.shape}")
    print(f"  AQI range: {y.min():.1f} to {y.max():.1f}")
    
    # Split data (80% train, 20% test)
    print("\nStep 4/5: Splitting data...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=False  # Don't shuffle time series!
    )
    
    print(f"‚úì Train set: {len(X_train)} samples")
    print(f"‚úì Test set: {len(X_test)} samples")
    
    # Scale features
    print("\nStep 5/5: Scaling features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("‚úì Features scaled using StandardScaler")
    
    print("\n" + "="*60)
    print("‚úÖ DATA PREPARATION COMPLETE")
    print("="*60)
    
    # Return data source info as extra parameter
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, available_features, data_source, duration_days


def train_models(X_train, X_test, y_train, y_test):
    """
    Train the 3 best ML models and compare performance.
    
    Models:
    - Random Forest: Ensemble of decision trees
    - XGBoost: Gradient boosting with regularization
    - LightGBM: Fast gradient boosting
    
    Args:
        X_train, X_test, y_train, y_test: Training and test data
    
    Returns:
        Dictionary with trained models and their metrics
    """
    print("\n" + "="*60)
    print("ü§ñ TRAINING 3 BEST MODELS")
    print("="*60 + "\n")
    
    models = {}
    results = []
    
    # 1. Random Forest
    print("1Ô∏è‚É£  Training Random Forest...")
    print("   Ensemble of 100 decision trees")
    rf = RandomForestRegressor(
        n_estimators=100,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    metrics_rf = calculate_metrics(y_test, y_pred_rf, "Random Forest")
    models['Random Forest'] = rf
    results.append(metrics_rf)
    print(f"   ‚úì RMSE: {metrics_rf['RMSE']}, MAE: {metrics_rf['MAE']}, R¬≤: {metrics_rf['R2']}")
    
    # 2. XGBoost
    if XGBOOST_AVAILABLE:
        print("\n2Ô∏è‚É£  Training XGBoost...")
        print("   Optimized gradient boosting")
        xgb_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        xgb_model.fit(X_train, y_train)
        y_pred_xgb = xgb_model.predict(X_test)
        metrics_xgb = calculate_metrics(y_test, y_pred_xgb, "XGBoost")
        models['XGBoost'] = xgb_model
        results.append(metrics_xgb)
        print(f"   ‚úì RMSE: {metrics_xgb['RMSE']}, MAE: {metrics_xgb['MAE']}, R¬≤: {metrics_xgb['R2']}")
    else:
        print("\n2Ô∏è‚É£  XGBoost not available - skipping")
    
    # 3. LightGBM
    if LIGHTGBM_AVAILABLE:
        print("\n3Ô∏è‚É£  Training LightGBM...")
        print("   Fast gradient boosting framework")
        lgb_model = lgb.LGBMRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            num_leaves=31,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            verbose=-1
        )
        lgb_model.fit(X_train, y_train)
        y_pred_lgb = lgb_model.predict(X_test)
        metrics_lgb = calculate_metrics(y_test, y_pred_lgb, "LightGBM")
        models['LightGBM'] = lgb_model
        results.append(metrics_lgb)
        print(f"   ‚úì RMSE: {metrics_lgb['RMSE']}, MAE: {metrics_lgb['MAE']}, R¬≤: {metrics_lgb['R2']}")
    else:
        print("\n3Ô∏è‚É£  LightGBM not available - skipping")
    
    print("\n" + "="*60)
    print(f"‚úÖ TRAINED {len(models)} MODELS")
    print("="*60)
    
    return models, results


def print_results_table(results):
    """
    Print comparison table of all models.
    
    Args:
        results: List of metric dictionaries
    """
    print("\n" + "="*60)
    print("üìä MODEL COMPARISON")
    print("="*60 + "\n")
    
    # Create DataFrame for better formatting
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values('RMSE')
    
    print(df_results.to_string(index=False))
    
    # Find best model
    best_idx = df_results['RMSE'].idxmin()
    best_model = df_results.loc[best_idx, 'model']
    best_rmse = df_results.loc[best_idx, 'RMSE']
    
    print("\n" + "="*60)
    print(f"üèÜ BEST MODEL: {best_model}")
    print(f"   RMSE: {best_rmse}")
    print("="*60)
    
    return best_model


def save_models(models, scaler, feature_names, project, data_source, duration_days):
    """
    Save best model and scaler locally and to Hopsworks.
    
    UPDATED: Saves metadata about data source and duration
    
    Args:
        models: Dictionary of trained models
        scaler: Fitted scaler
        feature_names: List of feature names
        project: Hopsworks project
        data_source: Description of data source
        duration_days: Number of days of data
    """
    print("\n" + "="*60)
    print("üíæ SAVING MODELS")
    print("="*60 + "\n")
    
    # Create models directory
    os.makedirs('models', exist_ok=True)
    
    # Save all models locally
    for name, model in models.items():
        filename = f"models/{name.replace(' ', '_').lower()}.pkl"
        joblib.dump(model, filename)
        print(f"‚úì Saved {name} to {filename}")
    
    # Save scaler
    joblib.dump(scaler, 'models/scaler.pkl')
    print(f"‚úì Saved scaler to models/scaler.pkl")
    
    # Save feature names
    joblib.dump(feature_names, 'models/feature_names.pkl')
    print(f"‚úì Saved feature names to models/feature_names.pkl")
    
    # UPDATED: Save metadata with data source info
    metadata = {
        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_source': data_source,
        'duration_days': duration_days,
        'meets_3_month_requirement': duration_days >= 90,
        'num_features': len(feature_names),
        'feature_names': feature_names
    }
    
    import json
    with open('models/metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"‚úì Saved metadata to models/metadata.json")
    
    print("\n‚úÖ All models saved locally in 'models/' directory")


def run_training_pipeline():
    """
    Run the complete training pipeline.
    """
    print("\n" + "="*70)
    print("üöÄ TRAINING PIPELINE STARTED")
    print(f"‚è∞ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # Connect to Hopsworks
    project = connect_to_hopsworks()
    if project is None:
        print("‚úó Cannot proceed without Hopsworks connection")
        return False
    
    # Load and prepare data (UPDATED to return data_source and duration)
    result = load_and_prepare_data(project)
    if result[0] is None:
        print("‚úó Cannot proceed without data")
        return False
    
    X_train, X_test, y_train, y_test, scaler, feature_names, data_source, duration_days = result
    
    # Train models
    models, results = train_models(X_train, X_test, y_train, y_test)
    
    # Print comparison table
    best_model_name = print_results_table(results)
    
    # Save models (UPDATED to include data source info)
    save_models(models, scaler, feature_names, project, data_source, duration_days)
    
    print("\n" + "="*70)
    print("‚úÖ TRAINING PIPELINE COMPLETED SUCCESSFULLY!")
    print("="*70 + "\n")
    
    print("üìù Summary:")
    print(f"  ‚Ä¢ Data source: {data_source}")
    print(f"  ‚Ä¢ Data duration: {duration_days} days")
    if duration_days >= 90:
        print(f"  ‚Ä¢ ‚úÖ Meets 3-month requirement!")
    print(f"  ‚Ä¢ Trained {len(models)} models")
    print(f"  ‚Ä¢ Best model: {best_model_name}")
    print(f"  ‚Ä¢ Models saved to: models/")
    print(f"  ‚Ä¢ Features used: {len(feature_names)}")
    
    print("\nüí° Next steps:")
    print("  1. Review model performance in the comparison table")
    print("  2. Run SHAP analysis for interpretability")
    print("  3. Build the dashboard to visualize predictions")
    
    return True


if __name__ == "__main__":
    try:
        success = run_training_pipeline()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚ùå Training pipeline failed with error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)